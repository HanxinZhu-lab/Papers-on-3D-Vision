# Papers-on-3D-Vision

## 3D Generation
 ### Text-to-3D
**CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation**   
Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, Kamal Rahimi Malekshan  
*CVPR, 2022*  
[[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Sanghi_CLIP-Forge_Towards_Zero-Shot_Text-To-Shape_Generation_CVPR_2022_paper.html)] [[Code](https://github.com/AutodeskAILab/Clip-Forge)]   
  
**Zero-Shot Text-Guided Object Generation with Dream Fields**  
Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole  
*CVPR, 2022*  
[[Paper](https://arxiv.org/abs/2112.01455)] [[Code](https://github.com/google-research/google-research/tree/master/dreamfields)]  
  
**DreamFusion: Text-to-3D using 2D Diffusion**  
Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall  
[[Paper](https://arxiv.org/abs/2209.14988)]

**CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language**  
Aditya Sanghi, Rao Fu, Vivian Liu, Karl Willis, Hooman Shayani, Amir Hosein Khasahmadi, Srinath Sridhar, Daniel Ritchie  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2211.01427)]
  
**Magic3D: High-Resolution Text-to-3D Content Creation**  
Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2211.10440)]  
  
**Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation**  
Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, Greg Shakhnarovich  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2212.00774)] [[Code](https://github.com/pals-ttic/sjc/)]

**Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models**  
Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Shenghua Gao  
*CVPR, 2023*  
[[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.html)]  

**Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures**  
Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2211.07600)] [[Code](https://github.com/eladrich/latent-nerf)]  

**Text2Scene: Text-driven Indoor Scene Stylization with Part-aware Details**  
Inwoo Hwang, Hyeonwoo Kim, Young Min Kim  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2308.16880)]
  
**DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model**  
Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun  
*Preprint, 2023*  
[[Paper](https://arxiv.org/abs/2304.02827)] [[Code](https://github.com/janeyeon/ditto-nerf-code)]  

**Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation**  
Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim  
*Preprint, 2023*  
[[Paper](https://arxiv.org/abs/2303.07937)] [[Code](https://github.com/KU-CVLAB/3DFuse)]  

**DreamBooth3D: Subject-Driven Text-to-3D Generation**  
Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, Varun Jampani  
*Preprint, 2023*  
[[Paper](https://arxiv.org/abs/2303.13508)]  

**ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation**  
Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu  
*Preprint, 2023*  
[[Paper](https://arxiv.org/abs/2305.16213)] [[Code](https://github.com/thu-ml/prolificdreamer)]  

**Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation**  
Rui Chen, Yongwei Chen, Ningxin Jiao, Kui Jia  
*ICCV, 2023*  
[[Paper](https://arxiv.org/abs/2303.13873)] [[Code](https://github.com/Gorilla-Lab-SCUT/Fantasia3D)]  

**MVDream: Multi-view Diffusion for 3D Generation**  
Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, Xiao Yang  
*Preprint, 2023*  
[[Paper](https://arxiv.org/abs/2308.16512)]

**EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior**  
Minda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng Li, Zeng Zhao, Zhipeng Hu, Changjie Fan, Xin Yu  
*Preprint, 2023*  
[[Paper](https://arxiv.org/abs/2308.13223)]

**3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion**  
Yu-Jhe Li, Kris Kitani  
*arXiv Preprint, 2023*  
[[Paper](https://arxiv.org/abs/2303.11938)]

**DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation**  
Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, Lei Zhang  
*arXiv Preprint*  
[[Paper](https://arxiv.org/abs/2306.12422)]  

**LERF: Language Embedded Radiance Fields**  
Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik  
*ICCV, 2023, Oral*  
[[Paper](https://arxiv.org/abs/2303.09553)] [[Code](https://github.com/kerrj/lerf)] [[Project Page](https://www.lerf.io/)]  

### Image-to-3D  
**NeRDi: Single-View NeRF Synthesis With Language-Guided Diffusion As General Image Priors**  
Congyue Deng, Chiyu "Max'' Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2212.03267)]  

**High-Fidelity 3D GAN Inversion by Pseudo-Multi-View Optimization**  
Jiaxin Xie, Hao Ouyang, Jingtan Piao, Chenyang Lei, Qifeng Chen  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2211.15662)] [[Code](https://github.com/jiaxinxie97/HFGI3D/)]  

**RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation**  
Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy J. Mitra, Paul Guerrero  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2211.09869)] [[Code](https://github.com/Anciukevicius/RenderDiffusion)]  

**HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images**  
Animesh Karnewar, Andrea Vedaldi, David Novotny, Niloy Mitra  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2303.16509)] [[Code](https://github.com/facebookresearch/holo_diffusion)]  

**Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior**  
Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, Dong Chen  
*ICCV, 2023*  
[[Paper](https://arxiv.org/abs/2303.14184)] [[Code](https://github.com/junshutang/Make-It-3D)]  

**Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction**  
Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, Hao Su  
*ICCV, 2023*  
[[Paper](https://arxiv.org/abs/2304.06714)] [[Code](https://github.com/Lakonik/SSDNeRF)]  

**Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation Using only Images**  
Cuican Yu, Guansong Lu, Yihan Zeng, Jian Sun, Xiaodan Liang, Huibin Li, Zongben Xu, Songcen Xu, Wei Zhang, Hang Xu  
*ICCV, 2023*  
[[Paper](https://arxiv.org/abs/2308.16758)]  

**HoloFusion: Towards Photo-realistic 3D Generative Modeling**  
Animesh Karnewar, Niloy J. Mitra, Andrea Vedaldi, David Novotny  
*ICCV,2023*  
[[Paper](https://arxiv.org/abs/2308.14244)]  

**Distribution-Aligned Diffusion for Human Mesh Recovery**  
Lin Geng Foo, Jia Gong, Hossein Rahmani, Jun Liu  
*ICCV, 2023*  
[[Paper](https://arxiv.org/abs/2308.13369)]

**BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models**  
Yao Wei, George Vosselman, Michael Ying Yang  
*ICCV Workshop, 2023*  
[[Paper](https://arxiv.org/abs/2309.00158)]

**DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model**  
Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun  
*Preprint, 2023*  
[[Paper](https://arxiv.org/abs/2304.02827)] [[Code](https://github.com/janeyeon/ditto-nerf-code)]

:+1: **SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction**  
Zhizhuo Zhou, Shubham Tulsiani  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2212.00792)]  [[Code](https://github.com/zhizdev/sparsefusion)]  [[Project Page](https://sparsefusion.github.io/)]   

**NOVEL VIEW SYNTHESIS WITH DIFFUSION MODELS**  
Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, Mohammad Norouzi  
*arXiv Preprint*  
[[Paper](https://arxiv.org/pdf/2210.04628.pdf)] [[Project Page](https://3d-diffusion.github.io/)]  

**3D-aware Image Generation using 2D Diffusion Models**  
Jianfeng Xiang, Jiaolong Yang, Binbin Huang, Xin Tong  
*arXiv Preprint*  
[[Paper](https://arxiv.org/abs/2303.17905)] [[Project Page](https://jeffreyxiang.github.io/ivid/)]  

:+1: **Efficient Geometry-aware 3D Generative Adversarial Networks**  
Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein  
*CVPR, 2022*  
[[Paper](https://arxiv.org/abs/2112.07945)] [[Code](https://github.com/NVlabs/eg3d)] [[Project Page](https://nvlabs.github.io/eg3d/)]  

**GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images**  
Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, Sanja Fidler  
*NeurIPS, 2022*  
[[Paper](https://nv-tlabs.github.io/GET3D/assets/paper.pdf)] [[Code](https://github.com/nv-tlabs/GET3D)] [[Project Page](https://nv-tlabs.github.io/GET3D/)]  

:+1: **One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization**  
Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, Hao Su  
*NeurIPS, 2023*  
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/4683beb6bab325650db13afd05d1a14a-Paper-Conference.pdf)] [[Code](https://github.com/One-2-3-45/One-2-3-45)] [[Project Page](https://one-2-3-45.github.io/)] 

:+1: **One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion**  
Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, Hao Su  
*arXiv Preprint*   
[[Paper](https://arxiv.org/abs/2311.07885)] [[Code](https://github.com/SUDO-AI-3D/One2345plus)] [[Project Page](https://sudo-ai-3d.github.io/One2345plus_page/)] 

**Wonder3D: Single Image to 3D using Cross-Domain Diffusion**   
Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, Wenping Wang  
*arXiv Preprint*   
[[Paper](https://arxiv.org/abs/2310.15008)] [[Code](https://github.com/xxlong0/Wonder3D)] [[Project Page](https://www.xxlong.site/Wonder3D//)] 

**Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting**  
Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Munan Ning, Li Yuan  
*arXiv Preprint*   
[[Paper](https://arxiv.org/abs/2312.13271)] [[Project Page](https://pku-yuangroup.github.io/repaint123/)]  


## 3D Editing
**Edit-DiffNeRF: Editing 3D Neural Radiance Fields using 2D Diffusion Model**  
Lu Yu, Wei Xiang, Kang Han  
*arXiv Preprint*  
[[Paper](https://arxiv.org/abs/2306.09551)]  

**DiffRF: Rendering-Guided 3D Radiance Field Diffusion**  
Norman M ̈ uller, Yawar Siddiqui, Lorenzo Porzi, Lorenzo Porzi, Samuel Rota Bul` o, Peter Kontschieder, Matthias Nießner1  
*CVPR, 2023, Highlight*  
[[Paper](https://arxiv.org/abs/2212.01206)]  [[Project Page](https://sirwyver.github.io/DiffRF/)]   

**CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields**  
Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao  
*CVPR, 2022*  
[[Paper](https://arxiv.org/abs/2112.05139)]  [[Code](https://github.com/cassiePython/CLIPNeRF)]  [[Project Page](https://cassiepython.github.io/clipnerf/)]   

**Editing Conditional Radiance Fields**  
Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell  
*ICCV, 2021*  
[[Paper](http://editnerf.csail.mit.edu/paper.pdf)]  [[Code](https://github.com/stevliu/editnerf)]  [[Project Page](http://editnerf.csail.mit.edu/)]   

**Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions**  
Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa  
*ICCV, 2023*  
[[Paper](https://arxiv.org/abs/2303.12789)]  [[Code](https://github.com/ayaanzhaque/instruct-nerf2nerf)]  [[Project Page](https://instruct-nerf2nerf.github.io/)]   

**Local 3D Editing via 3D Distillation of CLIP Knowledge**  
Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo  
*CVPR, 2023*  
[[Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Hyung_Local_3D_Editing_via_3D_Distillation_of_CLIP_Knowledge_CVPR_2023_paper.pdf)]  

**Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation**  
Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, Yueqi Duan  
*arXiv Preprint*   
[[Paper](https://arxiv.org/abs/2403.09625)]  

**Gaussian Splatting in Style**  
Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers  
*arXiv Preprint*   
[[Paper](https://arxiv.org/abs/2403.08498)]

## 3D Reconstruction
:+1: **Generalizable Patch-Based Neural Rendering**  
Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia  
*ECCV, 2022, Oral*  
[[Paper](https://arxiv.org/abs/2207.10662)]  [[Code](https://github.com/google-research/google-research/tree/master/gen_patch_neural_rendering)]  [[Project Page](https://mohammedsuhail.net/gen_patch_neural_rendering/)]  

:+1: **Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields**  
Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, Yuewen Ma  
*ICCV, 2023, Oral*  
[[Paper](https://arxiv.org/abs/2307.11335)]  [[Code](https://github.com/wbhu/Tri-MipRF)]  [[Project Page](https://wbhu.github.io/projects/Tri-MipRF/)]   

**Neuralangelo: High-Fidelity Neural Surface Reconstruction**  
Zhaoshuo Li, Thomas Müller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, Chen-Hsuan Lin  
*CVPR, 2023*
[[Paper](https://arxiv.org/abs/2306.03092)]  [[Code](https://github.com/nvlabs/neuralangelo)]  [[Project Page](https://research.nvidia.com/labs/dir/neuralangelo/)]   

**Bayes’ Rays: Uncertainty Quantification for Neural Radiance Fields**  
Lily Goli, Cody Reading, Silvia Sell ́ an, Alec Jacobson, Andrea Tagliasacchi  
*arXiv Preprint, 2024*  
[[Paper](https://arxiv.org/abs/2309.03185)]  [[Project Page](https://bayesrays.github.io/)]   

**3d gaussian splatting for real-time radiance field rendering**  
B Kerbl, G Kopanas, T Leimkühler, G Drettakis  
*SIGGRAPH 2023*  
[[Paper](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_low.pdf)]  [[Project Page](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)]   

**3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos**  
J Sun, H Jiao, G Li, Z Zhang, L Zhao, W Xing  
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/pdf/2403.01444.pdf)]  [[Project Page](https://sjojok.github.io/3dgstream/)]  
**3D Gaussian Model for Animation and Texturing**  
XE Wang, Z Sin  
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/ftp/arxiv/papers/2402/2402.19441.pdf)]  

**4k4d: Real-time 4d view synthesis at 4k resolution**  
Z Xu, S Peng, H Lin, G He, J Sun, Y Shen, H Bao, X Zhou  
*CVPR, 2024*
[[Paper](https://arxiv.org/pdf/2310.11448.pdf?trk=public_post_comment-text)]  [[Project Page](https://zju3dv.github.io/4k4d/)]  

**A New Split Algorithm for 3D Gaussian Splatting**  
Q Feng, G Cao, H Chen, TJ Mu, RR Martin, SM Hu  
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/pdf/2403.09143.pdf)]  

**Deblurring 3D Gaussian Splatting**  
B Lee, H Lee, X Sun, U Ali, E Park  
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/pdf/2401.00834.pdf)]  [[Project Page](https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/)]  

**Disentangled 3D Scene Generation with Layout Learning**  
D Epstein, B Poole, B Mildenhall, AA Efros, A Holynski
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/pdf/2402.16936.pdf)]  [[Project Page](https://dave.ml/layoutlearning/)]  

**DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization**  
J Li, J Zhang, X Bai, J Zheng, X Ning, J Zhou, L Gu  
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/pdf/2403.06912.pdf)]  [[Code](https://github.com/Fictionarry/DNGaussian)]  

**FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization**  
J Zhang, F Zhan, M Xu, S Lu, E Xing  
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/pdf/2403.06908.pdf)] 

**How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey**  
F Tosi, Y Zhang, Z Gong, E Sandström, S Mattoccia, MR Oswald, M Poggi  
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/pdf/2402.13255.pdf)]  

**IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation**  
L Melas-Kyriazi, I Laina, C Rupprecht, N Neverova, A Vedaldi, O Gafni, F Kokkinos  
*arXiv preprint, 2024* 
[[Paper](https://arxiv.org/pdf/2402.08682.pdf)]  

**TripoSR: Fast 3D Object Reconstruction from a Single Image**   
Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, Yan-Pei Cao  
*arXiv preprint*   
[[Paper](https://arxiv.org/abs/2403.02151)]  [[Code](https://github.com/VAST-AI-Research/TripoSR)]   

**Splatter Image: Ultra-Fast Single-View 3D Reconstruction**   
Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi   
*CVPR, 2024*  
[[Paper](https://arxiv.org/abs/2312.13150)]  [[Code](https://github.com/szymanowiczs/splatter-image)]   

:+1: **LRM: Large Reconstruction Model for Single Image to 3D**  
Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, Hao Tan  
*ICLR, 2024*   
[[Paper](https://arxiv.org/abs/2311.04400)]

**ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Real Image**  
Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, Jiajun Wu   
*arXiv preprint*  
[[Paper](https://arxiv.org/abs/2310.17994)]  [[Code](https://github.com/kylesargent/ZeroNVS)] 

**Mip-splatting: Alias-free 3d gaussian splatting**  
Z Yu, A Chen, B Huang, T Sattler, A Geiger  
*arXiv preprint, 2023* 
[[Paper](https://arxiv.org/pdf/2311.16493.pdf)]  

**Nope-nerf: Optimising neural radiance field with no pose prior**  
W Bian, Z Wang, K Li, JW Bian, VA Prisacariu  
*CVPR, 2023*  
[[Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.pdf)]  

**RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS**  
Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari   
*arXiv preprint, 2024*   
[[Paper](https://arxiv.org/pdf/2403.13806.pdf)]  [[Code](https://m-niemeyer.github.io/radsplat/)]  

**Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting**  
J Jung, J Han, H An, J Kang, S Park, S Kim  
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/pdf/2403.09413.pdf)]  [[Code](https://ku-cvlab.github.io/RAIN-GS/)]  

**Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing**  
Tian-Xing Xu, Wenbo Hu, Yu-Kun Lai, Ying Shan, Song-Hai Zhang
*arXiv preprint, 2024*  
[[Paper](https://arxiv.org/pdf/2403.10050.pdf)]  

## 3D Representation Learning
**ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding**  
Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese  
*CVPR, 2023*  
[[Paper](https://arxiv.org/abs/2212.05171)]  

**ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding**  
Le Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese  
*arXiv preprint*   
[[Paper](https://arxiv.org/abs/2305.08275)]  [[Code](https://github.com/salesforce/ULIP)]  

**OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding**  
Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, Hao Su  
*NeurIPS, 2023*  
[[Paper](https://arxiv.org/pdf/2305.10764.pdf)]  [[Code](https://github.com/Colin97/OpenShape_code)]  

**Uni3D: Exploring Unified 3D Representation at Scale**  
Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, Xinlong Wang   
*ICLR, 2024*    
[[Paper](https://arxiv.org/abs/2310.06773)]  [[Code](https://github.com/baaivision/Uni3D)]  


